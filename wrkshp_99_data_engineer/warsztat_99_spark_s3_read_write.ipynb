{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pierwsze polecenie uruchomi context sparka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ładujemy dodatkowe funckje z biblioteki Spark z modułu SQL\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15c19b",
   "metadata": {},
   "source": [
    "### Przygotwanie danych do ćwiczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wpisujemy nazwę bazy danych, którą utworzyliśmy w Hive\n",
    "nazwa_mojej_bazy_danych_w_hive='<wprowadz_swoja_nazwe_bazy_w_Hive>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7255f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wybieramy bazę danych dla Sparka w ramach, której będziemy pracować\n",
    "spark.sql('use {db}'.format(db=nazwa_mojej_bazy_danych_w_hive))\n",
    "print(\"Uzywana baza danych: {db}\".format(db=nazwa_mojej_bazy_danych_w_hive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowujemy mapowanie do obiektu, który będzie korzystał z tabeli w Hive\n",
    "posts = spark.table('posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowujemy obiekt na bazie posts, który:\n",
    "#   - będzie zawierał dodatkową kolumnę year - wykorzystamy ją do partycjonowania danych\n",
    "#   - wybieramy również inne kolumny, które nas interesują\n",
    "posts_with_year = posts \\\n",
    "    .withColumn('year', col('creationdate').substr(1, 4)) \\\n",
    "    .select('year','id','posttypeid','viewcount','body','title','owneruserid','answercount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzamy jak wygląda nowo przygotowana struktura\n",
    "posts_with_year.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobieramy 10 rekordów aby sprawdzić czy dane zgadzają się z oczekiwaniami\n",
    "posts_with_year.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b94226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzamy, ile lat mamy w naszym zbiorze - za chwilę będzie to nasz klucz partycjonowania\n",
    "posts_with_year.select('year').distinct().orderBy('year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ee343",
   "metadata": {},
   "source": [
    "### Zapis danych do bucketu w S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a81b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowujemy nazwe katalogu do którego zapiszemy dane na S3\n",
    "# Bucket jest wspólny dla wszystkich dlatego tworzymy dedykowana ścieżkę dla każdego z uczestników szkolenia\n",
    "path_on_s3_to_write_data = \"s3://kompleksowe-szkolenie-bigdata-write-exercise-v2/{db}/posts_with_year_partitions\" \\\n",
    "    .format(db=nazwa_mojej_bazy_danych_w_hive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34692f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sciezka do ktorej zapiszemy dane na S3: {path}\".format(path=path_on_s3_to_write_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacd90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykonujemy operację \"write\" w trybie \"overwrite\", partycjonując nasze dane przy zapisie za pomocą kolumny \"year\"\n",
    "# do nazwy katalogu przechowywanego w zmiennej \"path_on_s3_to_write_data\"\n",
    "# Dane zapisujemy używając formatu Parquet\n",
    "posts_with_year \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .parquet(path_on_s3_to_write_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6014d80",
   "metadata": {},
   "source": [
    "### Odczyt danych z bucketu S3 w Sparku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9944a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do nowego obiektu mapujemy ściężkę porzednio wykorzystywaną, aby zweryfikować czy nasze dane zostały poprawnie zapisane\n",
    "posts_with_year_s3_read = spark.read.parquet(path_on_s3_to_write_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzamy czy Spark poprawnie odczytał schemat danych - schemat danych jest zapisywany w plikach Parquet\n",
    "# Spark ma mechanizm automatycznego inferowania schemy z danych, które zna - w tym przypadku Parquet\n",
    "posts_with_year_s3_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzamy czy wszystkie lata, których oczekujemy, znajdują się w zbiorze\n",
    "posts_with_year_s3_read.select('year').distinct().orderBy('year').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615023f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzmy, czy dane poprawnie wyglądają\n",
    "posts_with_year_s3_read.filter(col('year')==2014).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946a56f",
   "metadata": {},
   "source": [
    "### Odtwarzanie tabeli w Hive na podstawie danych z S3\n",
    "\n",
    "Ten przykład można wykonać zarówno z poziomu Sparka jak i Hive - polecenia działają w obu środowiskach\n",
    "\n",
    "Tworzymy tabelę typu external oraz odtwarzamy w niej strukturę partycji na podstawie tego co zostało zapisane w S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb7329",
   "metadata": {},
   "source": [
    "**UWAGA!** W poniższym poleceniu musisz zmodyfikować \"location\" dla tabeli - ścieżka, którą należy podać została wygenerowana wcześniej - zmienna \"path_on_s3_to_write_data\" w kroku zapisywania danych do S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE EXTERNAL TABLE posts_with_year\n",
    "(\n",
    "    id long,\n",
    "    posttypeid int,\n",
    "    viewcount long,\n",
    "    body string,\n",
    "    title string,\n",
    "    owneruserid long,\n",
    "    answercount long\n",
    ")\n",
    "partitioned by (year integer)\n",
    "stored as parquet\n",
    "location '<wproadz_sceizke_zapisu_do_S3>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debde818",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "show partitions posts_with_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53da029",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "alter table posts_with_year recover partitions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e237723",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "show partitions posts_with_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb585921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select  * from posts_with_year where year = 2014 \n",
    "limit 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

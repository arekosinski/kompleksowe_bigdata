{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pierwsze polecenie wystartuje sesje pySparka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sprawdzamy dane wygenerowane w Hive w Sparku\")\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ustawiamy domyślną bazę danych Hive z której będziemy korzystać"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nazwa_mojej_bazy_danych_w_hive='nazwa_bazy_danych_zalozonej_w_hive'\n",
    "spark.sql('use {db}'.format(db=nazwa_mojej_bazy_danych_w_hive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wyświetlamy wszystkie tabele z naszej bazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show(200,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe rozwiązanie zadania 4 z Hive:\n",
    "Jaka jest ilość pytań zadawanych per rok? Jaka jest ilośc pytań z udzieloną odpowiedzią per rok ?\n",
    "Jaki jest procent pytań z odpowiedzią per rok? (Spróbuj wykonać to zadanie jednym zapytaniem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowujemy obiekt, który będzie korzystał z tabeli w Hive\n",
    "posts = spark.table('posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts \\\n",
    "    .filter(col('PostTypeId') == 1) \\\n",
    "    .withColumn('year', col('creationdate').substr(1, 4)) \\\n",
    "    .withColumn('is_acceptedanswerid', when(col('acceptedanswerid')!=0, 1).otherwise(0)) \\\n",
    "    .select('year','is_acceptedanswerid') \\\n",
    "    .groupBy(\"year\") \\\n",
    "    .agg( \\\n",
    "        sum(\"is_acceptedanswerid\").alias(\"sum_salary\"), \\\n",
    "        count('year').alias('cnt_year') \\\n",
    "    ) \\\n",
    "    .withColumn('per100', col('sum_salary') / col('cnt_year')*100 ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzamy, czy wyniki z zapytania SQL zwróci taki sam wynik\n",
    "spark.sql(\"\"\"\n",
    "select substr(posts.creationdate,1,4) as year,\n",
    "count(posts.id) as posts,\n",
    "sum(if(posts.acceptedanswerid != 0,1,0)) as answered_count,\n",
    "sum(if(posts.acceptedanswerid != 0,1,0))/count(posts.id) as anserser_percent\n",
    "from posts\n",
    "where posts.PostTypeId = 1\n",
    "group by substr(posts.creationdate,1,4)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe rozwiązanie problemu 6 z Hive:\n",
    "Wskaż użytkowników z najwyższą średnią oceną za udzielone odpowiedzi. Bierz pod uwagę tylko tych którzy mają więcej niż 100 odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    users_parquet.Id,\n",
    "    users_parquet.DisplayName,\n",
    "    count(posts.Id) AS Answers,\n",
    "    AVG(CAST(posts.Score AS float)) AS AverageAnswerScore\n",
    "FROM posts\n",
    "    INNER JOIN users_parquet ON users_parquet.Id = posts.OwnerUserId\n",
    "WHERE posts.PostTypeId = 2\n",
    "GROUP BY users_parquet.Id, users_parquet.DisplayName\n",
    "HAVING Count(Posts.Id) > 100\n",
    "ORDER BY AverageAnswerScore DESC\n",
    "limit 100\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowujemy obiekty, które będą korzystać z tabel w Hive\n",
    "posts = spark.table('posts')\n",
    "users = spark.table('users_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdźmy schemat tabeli posts\n",
    "posts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdźmy schemat tabeli users\n",
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users \\\n",
    "    .join(\n",
    "        posts.filter(posts.PostTypeId == 2), \\\n",
    "        users.id == posts.OwnerUserId \\\n",
    "    ) \\\n",
    "    .groupBy(\n",
    "        users.id, \\\n",
    "        users.displayname \\\n",
    "    ) \\\n",
    "    .agg( \\\n",
    "        count(posts.Id).alias('count_Id'), \\\n",
    "        avg(posts.Score).alias('avg_score') \\\n",
    "    ) \\\n",
    "    .filter(col('count_Id')>100) \\\n",
    "    .orderBy(col('avg_score').desc()) \\\n",
    "    .limit(100) \\\n",
    "    .show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
